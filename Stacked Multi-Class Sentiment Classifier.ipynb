{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AGDpvTeL9dsu"
   },
   "source": [
    "# Stacked Logistic Regression-UniDirectional LSTM and Ensemble Implementation for Yelp Dataset Multiclass Sentiment Analysis Classification\n",
    "\n",
    "**Platforms utilisied for testing/running this jupyter notebook file: Anaconda Jupyter Notebook and Google Colab (Optimal for training deep learning models using GPU)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents \n",
    "\n",
    "* [Preliminary Step](#Preliminary Step)\n",
    "    * [Mounting Google Drive](#Mounting Google Drive)\n",
    "* [1. Introduction](#Introduction)\n",
    "* [2. Load/Read in all required libraries](#libraries)\n",
    "* [3. Load/read in all datasets](#Load/read in all datasets)\n",
    "* [4. Exploratory Data Analysis (EDA)](#EDA)\n",
    "* [5. Text Preprocessing and Auxiliary/Helper Functions](#Text Preprocessing Auxiliary/Helper Functions)\n",
    "   * [5.1 Preprocessing Labeled Training Dataset](#Preprocessing Labeled Training Dataset)\n",
    "* [6. Logistic Regression Model Implementation](#Logistic Regression Model Implementation)\n",
    "   * [6.1 Fit Logistic Regression Model](#Fit Logistic Regression Model)\n",
    "* [7. Creating Augmented Dataset (Labeled Data + Predicted Unlabeled Data) total 650k](#Creating Augmented Dataset Labeled Data + Predicted Unlabeled Data total 650k)\n",
    "   * [7.1 Preprocessing Unlabeled Training Dataset](#Preprocessing Unlabeled Training Dataset)\n",
    "   * [7.2 Unlabeled Training Dataset Feature Extraction](#Unlabeled Training Dataset Feature Extraction)\n",
    "   * [7.3 Unlabeled Training Dataset Predictions](#Unlabeled Training Dataset Predictions)\n",
    "   * [7.4 Creating Augmented Training Dataset Labeled Data + Predicted Unlabeled Data](#Creating Augmented Training Dataset Labeled Data + Predicted Unlabeled Data)\n",
    "* [8. Unidirectional Long Short Term Memory (LSTM) Implementation](#Unidirectional Long Short Term Memory LSTM Implementation)\n",
    "   * [8.1 Load/Read in preprocessed Augmented Dataset and Test Dataset](#Load/Read in preprocessed Augmented Dataset and Test Dataset)\n",
    "   * [8.2 Tokenization for Undirectional LSTM](#Tokenization for Undirectional LSTM)\n",
    "   * [8.3 Augmented Training Dataset (Labeled Data + Predicted Unlabeled Data) Feature Extraction for ULSTM](#Augmented Training Dataset Labeled Data + Predicted Unlabeled Data Feature Extraction for ULSTM)\n",
    "   * [8.4 Fit UniDirectional Long Short Term Memory (LSTM) Model on Augmented Dataset](#Fit UniDirectional LSTM Model on Augmented Dataset)\n",
    "* [9. Augmented Dataset Recreation from predicting unlabeled dataset with LSTM model  ](#Augmented Dataset Recreation from predicting unlabeled dataset with LSTM model)\n",
    "   * [9.1 Augmented Dataset Recreation by utilising Unidirectional LSTM Model to Predict Unlabeled Data](#Augmented Dataset Recreation by utilising Unidirectional LSTM Model to Predict Unlabeled Data)\n",
    "      * [9.1.1 Export to CSV repredicted Augmented Dataset with original unpreprocessed 650K text column](#Export to CSV repredicted Augmented Dataset with original unpreprocessed 650K text column)\n",
    "      * [9.1.2 Export to CSV repredicted Augmented Dataset with original preprocessed 650K text column](#Export to CSV repredicted Augmented Dataset with original preprocessed 650K text column)\n",
    "* [10. Stacked Ensemble Model with implementation variants of LSTM, GRU and CNN Models](#Stacked Ensemble Model with implementation variants of LSTM, GRU and CNN Models)\n",
    "   * [10.1 Load/Read in all required deep learning libraries and packages](#Load/Read in all required deep learning libraries and packages)\n",
    "   * [10.2 Load/Read in Augmented Dataset without preprocessed reviews and Testing Dataset](#Load/Read in Augmented Dataset without preprocessed reviews and Testing Dataset)\n",
    "   * [10.3 Tokenization](#Tokenization)\n",
    "   * [10.4 FastText Word Embedding Matrix](#FastText Word Embedding Matrix)\n",
    "      * [10.4.1 Download FastText](#Download FastText)\n",
    "      * [10.4.2 Unzip FastText Word Embeddings](#Unzip FastText Word Embeddings)\n",
    "      * [10.4.3 Constructing and Implementing FastText Word Embedding Matrix](#Constructing and Implementing FastText Word Embedding Matrix)\n",
    "   * [10.5 One Hot Encode Training Dataset](#One Hot Encode Training Dataset)\n",
    "   * [10.6 Ensemble Models Implementation](#Ensemble Models Implementation)\n",
    "   * [10.7 Loading Optimal Model Implementations](#Loading Optimal Model Implementations)\n",
    "* [11. Ensemble Testing Dataset Predictions](#Ensemble Testing Dataset Predictions)\n",
    "* [12. Create and export output file predict_label.csv](#Create and export output file predict_label.csv)\n",
    "* [References](#references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Step \n",
    "<a id=\"Preliminary Step\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mounting Google Drive\n",
    "<a id=\"Mounting Google Drive\"></a>\n",
    "\n",
    "As a preliminary step the as this `jupyter notebook file` **Stacked Multi-Class Sentiment Classifier.ipynb**, is executed inside **Google Colab**, thus, all the required datasets will be load into **Google Drive**. Therefore, the following block of code mounts the respective users Google Drive into the `jupyter notebook file`, thus, allowing easy integration in implementing and using all datasets and files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 26197,
     "status": "ok",
     "timestamp": 1572104509658,
     "user": {
      "displayName": "Jeffery Liu",
      "photoUrl": "",
      "userId": "10738336117249433609"
     },
     "user_tz": -660
    },
    "id": "05r-r0159gJm",
    "outputId": "4f1495d0-a983-48b6-be65-bbb4cfb02a24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4168,
     "status": "ok",
     "timestamp": 1572104511915,
     "user": {
      "displayName": "Jeffery Liu",
      "photoUrl": "",
      "userId": "10738336117249433609"
     },
     "user_tz": -660
    },
    "id": "ky5a0kFp-CdZ",
    "outputId": "f1193f78-3e8f-413e-d98d-6b6217e2f407"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeled_data.csv\n",
      "logistic_200KAugmented_dataset.csv\n",
      "logistic_650KAugmented_dataset.csv\n",
      "logistic_GloVeEmbed_PreProcSentences_650KAugmented_dataset.csv\n",
      "logistic_originalSent_650KAugmented_dataset.csv\n",
      "logistic_origSent_200KAugmented_dataset.csv\n",
      "\u001b[0m\u001b[01;34mmodels\u001b[0m/\n",
      "test_data.csv\n",
      "ULMFit_model.pkl\n"
     ]
    }
   ],
   "source": [
    "ls /content/gdrive/'My Drive'/yelp-multiclass-datasets/yelp-multiclass-sentiment-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6OhoL-xL9ds8"
   },
   "source": [
    "## 2. Load/Read in all required libraries  \n",
    "<a id=\"Load/Read in Libraries\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2805,
     "status": "ok",
     "timestamp": 1572104514161,
     "user": {
      "displayName": "Jeffery Liu",
      "photoUrl": "",
      "userId": "10738336117249433609"
     },
     "user_tz": -660
    },
    "id": "wLFw-TOL9ds-",
    "outputId": "93ecd127-5051-4565-e245-831dfdcd51e3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import matplotlib.pyplot as plt \n",
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import SpatialDropout1D\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras import utils\n",
    "from keras.models import load_model\n",
    "from nltk import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load/Read in all datasets\n",
    "<a id=\"Load/Read in all datasets\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data = pd.read_csv(\"/content/gdrive/My Drive/labeled_data.csv\") # training_dataset 50000 x 2 with columns [text, label]\n",
    "unlabeled_data = pd.read_csv(\"/content/gdrive/My Drive/unlabeled_data.csv\") # unlabeled training data 600000 with column [text]\n",
    "test_data = pd.read_csv(\"/content/gdrive/My Drive/test_data.csv\") # testing dataset 50000 x 2 with columns [text_id, text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6FW_QpE69dtS"
   },
   "source": [
    "## 4. Exploratory Data Analysis (EDA)\n",
    "<a id=\"Exploratory Data Analysis\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2H3haXVk9dtU",
    "outputId": "67417d28-6995-471d-c703-5eb984a42182"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>The new rule is - \\r\\nif you are waiting for a...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Flirted with giving this two stars, but that's...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>I was staying at planet Hollywood across the s...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Food is good but prices are super expensive.  ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Worse company to deal with they do horrible wo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  The new rule is - \\r\\nif you are waiting for a...      4\n",
       "1  Flirted with giving this two stars, but that's...      3\n",
       "2  I was staying at planet Hollywood across the s...      5\n",
       "3  Food is good but prices are super expensive.  ...      2\n",
       "4  Worse company to deal with they do horrible wo...      1"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I0KtBoj79dtZ",
    "outputId": "f362f359-fe54-43b8-c711-d1237628b2cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.20244\n",
       "5    0.20036\n",
       "2    0.19916\n",
       "4    0.19912\n",
       "3    0.19892\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#polarity\n",
    "labeled_data.label.value_counts() / labeled_data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NbWpPblT9dtd"
   },
   "source": [
    "## 5. Text Preprocessing and Auxiliary/Helper Functions\n",
    "<a id=\"Text Preprocessing and Auxiliary/Helper Functions\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m4PezrcQ9dtf"
   },
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "\n",
    "def process_text(text):\n",
    "    #pre rules\n",
    "    punctuation = string.punctuation + '\\n\\n\\r';\n",
    "    punc_replace = ''.join([' ' for s in punctuation]);\n",
    "    doco_clean = text.replace('-', ' ');\n",
    "    doco_alphas = re.sub(r'\\W +', ' ', doco_clean)\n",
    "    trans_table = str.maketrans(punctuation, punc_replace)\n",
    "    doco_clean = ' '.join([word.translate(trans_table) for word in doco_alphas.split(' ')]); \n",
    "    doco_clean = doco_clean.split(' '); \n",
    "    doco_clean = [ps.stem(word) for word in doco_clean]; # try stemming\n",
    "    doco_clean = [word.lower() for word in doco_clean if len(word) > 0]\n",
    "    \n",
    "    \n",
    "    return doco_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QUAn5Rp29dtj"
   },
   "outputs": [],
   "source": [
    "def tokenize(string):\n",
    "    res = [word for word in nltk.word_tokenize(string) if word and not re.search(pattern=r\"\\s+\", string=word)]\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "InDRB4rT9dtq"
   },
   "source": [
    "### 5.1 Preprocessing Labeled Training Dataset \n",
    "<a id=\"Preprocessing Labeled Training Dataset\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U2aFjiJJ9dtr"
   },
   "outputs": [],
   "source": [
    "labeled_reviews = []\n",
    "\n",
    "for line in range(labeled_data.shape[0]):\n",
    "    labeled_reviews.append(labeled_data.iloc[line,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z1SAnZSZ9dtw"
   },
   "outputs": [],
   "source": [
    "cleaned_review_text = [process_text(review) for review in labeled_reviews];\n",
    "cleaned_labeled_sentences = [' '.join(r) for r in cleaned_review_text]\n",
    "\n",
    "labeled_data['text'] = cleaned_labeled_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MhY5iD6a9dt0",
    "outputId": "42dd9617-d7b7-44c5-a822-6e2c7544f819"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>the new rule is if you are wait for a tabl whi...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>flirt with give thi two star but that s a pret...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>i wa stay at planet hollywood across the stree...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>food is good but price are super expens 8 buck...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>wors compani to deal with they do horribl work...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  the new rule is if you are wait for a tabl whi...      4\n",
       "1  flirt with give thi two star but that s a pret...      3\n",
       "2  i wa stay at planet hollywood across the stree...      5\n",
       "3  food is good but price are super expens 8 buck...      2\n",
       "4  wors compani to deal with they do horribl work...      1"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qIZu7GME9dt4"
   },
   "source": [
    "## 6. Logistic Regression Model Implementation\n",
    "<a id=\"Logistic Regression Model Implementation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XsIuQwlj9dt6"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, valid_df = train_test_split(labeled_data, test_size=0.162, random_state = 1412)\n",
    "\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "valid_df = valid_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kOyD-hCo9dt-",
    "outputId": "3bc0f714-a9c4-4e99-f43b-0fd52df95316"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>i got hook with the great workout but after th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>serious terribl custom servic if you need actu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>i have tri thi place twice i like it veri much...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>some of the worst pizza i ve ever had we use a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>i don t know how the commiss work at the att s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  i got hook with the great workout but after th...      1\n",
       "1  serious terribl custom servic if you need actu...      1\n",
       "2  i have tri thi place twice i like it veri much...      5\n",
       "3  some of the worst pizza i ve ever had we use a...      1\n",
       "4  i don t know how the commiss work at the att s...      1"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1gvT5YQx9duD"
   },
   "outputs": [],
   "source": [
    "#dependent variables \n",
    "# run this code block if there's split for training dataset and validation dataset\n",
    "y_train = train_df['label']\n",
    "y_valid = valid_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yNljSkK29duH",
    "outputId": "947580e0-4a6f-49a0-f435-9b46e80ee029"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((41900, 66810), (8100, 66810))"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#text features\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "tfidf = TfidfVectorizer(tokenizer=tokenize, ngram_range=(1,3), min_df=20, sublinear_tf=True)\n",
    "tfidf_fit = tfidf.fit(labeled_data['text'])\n",
    "text_train = tfidf_fit.transform(train_df['text'])\n",
    "text_valid = tfidf_fit.transform(valid_df['text'])\n",
    "text_train.shape, text_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FeP8NbuJ9duM"
   },
   "source": [
    "### 6.1 Fit Logistic Regression Model \n",
    "<a id=\"Fit Logistic Regression Model \"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y34J2HBG9duN",
    "outputId": "1478a49e-99f0-41aa-e60e-6d315650dbaa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=2.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit logistic regression models\n",
    "model = LogisticRegression(C=2., penalty='l2', solver='liblinear', dual=False, multi_class='ovr') # liblinear only support ovr\n",
    "model.fit(text_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lL0jqm-B9duS",
    "outputId": "2d54d794-02ac-47ac-cb6d-6d55ed880e60"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6175308641975309"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run if there's validation dataset\n",
    "model.score(text_valid,y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oyUBdOZr9dvR"
   },
   "source": [
    "## 7. Creating Augmented Dataset (Labeled Data + Predicted Unlabeled Data) total 650k \n",
    "<a id=\"Creating Augmented Dataset Labeled Data + Predicted Unlabeled Data total 650k\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "obphOWY79dvS"
   },
   "source": [
    "### 7.1 Preprocessing Unlabeled Training Dataset \n",
    "<a id=\"Preprocessing Unlabeled Training Dataset \"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G5L1mLwI9dvT"
   },
   "outputs": [],
   "source": [
    "# unlabeled_data_subset (total 148K dataset 100K preedicted dataset + 48K labeled dataset)\n",
    "test_df = unlabeled_data[0:500001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T8Jl9o4F9dvX"
   },
   "outputs": [],
   "source": [
    "unlabeled_reviews = []\n",
    "\n",
    "for line in range(test_df.shape[0]):\n",
    "    unlabeled_reviews.append(test_df.iloc[line,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8mWlTesF9dve"
   },
   "outputs": [],
   "source": [
    "cleaned_unlabeled_review_text = [process_text(review) for review in unlabeled_reviews];\n",
    "cleaned_unlabeled_sentences = [' '.join(r) for r in cleaned_unlabeled_review_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p6-bnoKE9dvp"
   },
   "outputs": [],
   "source": [
    "test_df['text'] = cleaned_unlabeled_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pVq5hiBs9dvx"
   },
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uNv6SzfO9dv0"
   },
   "source": [
    "### 7.2 Unlabeled Training Dataset Feature Extraction \n",
    "<a id=\"Unlabeled Training Dataset Feature Extraction\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tGwHQSjO9dv1"
   },
   "outputs": [],
   "source": [
    "#text features\n",
    "tfidf = TfidfVectorizer(tokenizer=tokenize, ngram_range=(1,3), min_df=20, sublinear_tf=True)\n",
    "tfidf_fit = tfidf.fit(labeled_data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gfMC4wT09dv6"
   },
   "outputs": [],
   "source": [
    "unlabeled_data_Xtrain = tfidf_fit.transform(test_df['text'])\n",
    "unlabeled_data_Xtrain.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ocfp4Tlk9dwC"
   },
   "source": [
    "### 7.3 Unlabeled Training Dataset Predictions\n",
    "<a id=\"Unlabeled Training Dataset Predictions\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uaYa74FC9dwD"
   },
   "outputs": [],
   "source": [
    "unlabeled_preds = model.predict(unlabeled_data_Xtrain)\n",
    "\n",
    "unlabeled_predicted_df = pd.DataFrame({\"text\" : test_df['text'],\n",
    "                                       \"label\" : unlabeled_preds})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FaujXaU99dwG"
   },
   "source": [
    "### 7.4 Creating Augmented Training Dataset (Labeled Data + Predicted Unlabeled Data)\n",
    "<a id=\"Creating Augmented Training Dataset Labeled Data + Predicted Unlabeled Data\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8Lgvhn9q9dwI"
   },
   "outputs": [],
   "source": [
    "augmented_training_data = labeled_data.append(unlabeled_predicted_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jZdNT4Oz9dwL"
   },
   "outputs": [],
   "source": [
    "# reset dataframe index to start from 0 \n",
    "augmented_train_df = augmented_training_data.reset_index(drop=True)\n",
    "augmented_train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KB07DvJ99dwP"
   },
   "outputs": [],
   "source": [
    "augmented_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 Exporting Augmented Training Dataset to CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_train_df.to_csv(\n",
    "    '/content/gdrive/My Drive/yelp-multiclass-datasets/yelp-multiclass-sentiment-dataset/logistic_650KAugmented_dataset.csv', \n",
    "    index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GB2oSZRn9dwX"
   },
   "source": [
    "## 8. Unidirectional Long Short Term Memory (LSTM) Implementation \n",
    "<a id=\"Unidirectional Long Short Term Memory (LSTM) Implementation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Load/Read in preprocessed Augmented Dataset and Test Dataset \n",
    "<a id=\"Load/Read in preprocessed Augmented Dataset and Test Dataset\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\n",
    "    \"/content/gdrive/My Drive/yelp-multiclass-datasets/yelp-multiclass-sentiment-dataset/logistic_650KAugmented_dataset.csv\")\n",
    "\n",
    "test_data = pd.read_csv(\n",
    "    \"/content/gdrive/My Drive/yelp-multiclass-datasets/yelp-multiclass-sentiment-dataset/test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all NaN rows\n",
    "augmented_training_data = train_data.dropna()\n",
    "\n",
    "# check that all NaN is dropped\n",
    "augmented_training_data[augmented_training_data.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Tokenization for Undirectional LSTM \n",
    "<a id=\"Tokenization for Undirectional LSTM\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wxe_cBAv9dwe"
   },
   "outputs": [],
   "source": [
    "augmented_clean_sentences = list(augmented_training_data.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4UfZI1Uf9dwj"
   },
   "outputs": [],
   "source": [
    "# Use a Keras Tokenizer and fit on the sentences for all cleaned labeled sentences\n",
    "\n",
    "tokenizer = Tokenizer();\n",
    "tokenizer.fit_on_texts(augmented_clean_sentences);\n",
    "text_sequences = np.array(tokenizer.texts_to_sequences(augmented_clean_sentences));\n",
    "sequence_dict = tokenizer.word_index;\n",
    "word_dict = dict((num, val) for (val, num) in sequence_dict.items());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 158705,
     "status": "ok",
     "timestamp": 1572104744657,
     "user": {
      "displayName": "Jeffery Liu",
      "photoUrl": "",
      "userId": "10738336117249433609"
     },
     "user_tz": -660
    },
    "id": "7JFZic9e9dws",
    "outputId": "480c34e1-4ee9-4cd3-c9b1-4689aa438886"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 144521 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# The maximum number of words to be used. (most frequent) which is the total tokens + 1 = 144522\n",
    "MAX_NB_WORDS = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Max number of words in each complaint.\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "# This is fixed.\n",
    "EMBEDDING_DIM = 300\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='', lower=False)\n",
    "tokenizer.fit_on_texts(augmented_clean_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AjKEBI4H9dxH"
   },
   "source": [
    "### 8.3 Augmented Training Dataset (Labeled Data + Predicted Unlabeled Data) Feature Extraction for ULSTM\n",
    "<a id=\"Augmented Training Dataset Labeled Data Predicted Unlabeled Data Feature Extraction for ULSTM\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 51996,
     "status": "ok",
     "timestamp": 1572104814703,
     "user": {
      "displayName": "Jeffery Liu",
      "photoUrl": "",
      "userId": "10738336117249433609"
     },
     "user_tz": -660
    },
    "id": "88OVJF6f9dxI",
    "outputId": "c9c3923a-0afb-42d0-faec-73ecfd4c8e26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (649988, 250)\n"
     ]
    }
   ],
   "source": [
    "X = tokenizer.texts_to_sequences(augmented_clean_sentences)\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 51470,
     "status": "ok",
     "timestamp": 1572104814705,
     "user": {
      "displayName": "Jeffery Liu",
      "photoUrl": "",
      "userId": "10738336117249433609"
     },
     "user_tz": -660
    },
    "id": "PzaT_92q9dxL",
    "outputId": "3f35de84-9f39-4531-c3d3-5170aa9ecca5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of label tensor: (649988, 5)\n"
     ]
    }
   ],
   "source": [
    "Y = pd.get_dummies(augmented_training_data['label']).values\n",
    "print('Shape of label tensor:', Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AanczZdU9dxP"
   },
   "source": [
    "### 8.4 Fit UniDirectional Long Short Term Memory (LSTM) Model on Augmented Dataset \n",
    "<a id=\"Fit UniDirectional LSTM Model on Augmented Dataset\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5ySLObENPtsP"
   },
   "outputs": [],
   "source": [
    "from keras.layers import CuDNNLSTM\n",
    "from keras.layers import CuDNNGRU\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9920888,
     "status": "ok",
     "timestamp": 1572125629373,
     "user": {
      "displayName": "Jeffery Liu",
      "photoUrl": "",
      "userId": "10738336117249433609"
     },
     "user_tz": -660
    },
    "id": "4x3oqkus9dxQ",
    "outputId": "4e71350d-aa0c-4c11-adcc-64dab4e4002d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 250, 300)          43356600  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_5 (Spatial (None, 250, 300)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_9 (CuDNNLSTM)     (None, 250, 150)          271200    \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_10 (CuDNNLSTM)    (None, 150)               181200    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 5)                 755       \n",
      "=================================================================\n",
      "Total params: 43,809,755\n",
      "Trainable params: 43,809,755\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 636988 samples, validate on 13000 samples\n",
      "Epoch 1/3\n",
      "636988/636988 [==============================] - 3323s 5ms/step - loss: 0.6268 - acc: 0.7417 - val_loss: 0.4745 - val_acc: 0.8062\n",
      "Epoch 2/3\n",
      "636988/636988 [==============================] - 3307s 5ms/step - loss: 0.4518 - acc: 0.8196 - val_loss: 0.4485 - val_acc: 0.8153\n",
      "Epoch 3/3\n",
      "636988/636988 [==============================] - 3289s 5ms/step - loss: 0.3882 - acc: 0.8484 - val_loss: 0.4431 - val_acc: 0.8198\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, 300, input_length=X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(CuDNNLSTM(150, return_sequences=True))\n",
    "model.add(CuDNNLSTM(150))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "epochs = 3\n",
    "batch_size = 32\n",
    "\n",
    "history = model.fit(X, Y, epochs=epochs, \n",
    "                    batch_size=batch_size, validation_split=0.02,\n",
    "                    callbacks=[EarlyStopping(monitor='val_loss', patience=1, min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XY1YBnkC9dxV"
   },
   "outputs": [],
   "source": [
    "# save model \n",
    "model.save('Unidirection LSTM.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fY7BHPms9dxY"
   },
   "outputs": [],
   "source": [
    "# load model \n",
    "model = load_model('Unidirection LSTM.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Augmented Dataset Recreation from predicting unlabeled dataset with LSTM model  \n",
    "<a id=\"Augmented Dataset Recreation from predicting unlabeled dataset with LSTM model  \"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uevO19h1Ka0B"
   },
   "source": [
    "### 9.1 Augmented Dataset Recreation by utilising Unidirectional LSTM Model to Predict Unlabeled Data\n",
    "<a id=\"Augmented Dataset Recreation by utilising Unidirectional LSTM Model to Predict Unlabeled Data\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 610936,
     "status": "ok",
     "timestamp": 1572128402379,
     "user": {
      "displayName": "Jeffery Liu",
      "photoUrl": "",
      "userId": "10738336117249433609"
     },
     "user_tz": -660
    },
    "id": "cCiiqeP9Dmjx",
    "outputId": "d3db85a4-ca44-48e6-c7e2-c9c5f82ddbde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (600000, 250)\n"
     ]
    }
   ],
   "source": [
    "### Repredict the unlabeled dataset and recreate the augmented dataset with LSTM model which has increased accuracy of ~62%\n",
    "\n",
    "trainn_data = pd.read_csv(\n",
    "    \"/content/gdrive/My Drive/yelp-multiclass-datasets/yelp-multiclass-sentiment-dataset/logistic_650KAugmented_dataset.csv\")\n",
    "\n",
    "preproc_label_data = trainn_data.loc[0:49999]\n",
    "\n",
    "predicted_unlabel_data = trainn_data.loc[50000:].fillna('')\n",
    "\n",
    "predicted_clean_unlabelsentences = list(predicted_unlabel_data.text)\n",
    "\n",
    "X_test = tokenizer.texts_to_sequences(predicted_clean_unlabelsentences)\n",
    "padded_X = pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', padded_X.shape)\n",
    "\n",
    "repredict_unlabeled = model.predict(padded_X, verbose=2)\n",
    "\n",
    "# output label classes (polarity levels)\n",
    "labels = [1, 2, 3, 4, 5]\n",
    "\n",
    "predicted_labels = []\n",
    "\n",
    "for prediction in repredict_unlabeled:\n",
    "    predicted_labels.append(labels[np.argmax(prediction)])\n",
    "    \n",
    "repredict_unlabel_data = pd.DataFrame({'text' : predicted_unlabel_data['text'],\n",
    "                                       'label' : predicted_labels})\n",
    "\n",
    "repredicted_augment_data = preproc_label_data.append(repredict_unlabel_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1.1 Export to CSV repredicted Augmented Dataset with original unpreprocessed 650K text column \n",
    "<a id=\"Export to CSV repredicted Augmented Dataset with original unpreprocessed 650K text column\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_sent_data = pd.read_csv(\n",
    "    \"/content/gdrive/My Drive/yelp-multiclass-datasets/yelp-multiclass-sentiment-dataset/logistic_originalSent_650KAugmented_dataset.csv\")\n",
    "\n",
    "del repredicted_augment_df['text']\n",
    "\n",
    "repredicted_augment_df['text'] = list(original_sent_data.text)\n",
    "\n",
    "repredicted_augment_df.to_csv(\n",
    "    '/content/gdrive/My Drive/650K_LSTM_Repredicted_originalSent_Augment_Dataset.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1.2 Export to CSV repredicted Augmented Dataset with original preprocessed 650K text column \n",
    "<a id=\"Export to CSV repredicted Augmented Dataset with original preprocessed 650K text column\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repredicted_augment_df = repredicted_augment_data.reset_index(drop=True) \n",
    "\n",
    "repredicted_augment_df.to_csv('/content/gdrive/My Drive/650K LSTM Repredicted Augment Dataset.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iEnnbXyc8bhL"
   },
   "source": [
    "## 10. Stacked Ensemble Model with implementation variants of LSTM, GRU and CNN Models\n",
    "<a id=\"Stacked Ensemble Model with implementation variants of LSTM, GRU and CNN Models\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1 Load/Read in all required deep learning libraries and packages \n",
    "<a id=\"Load/Read in all required deep learning libraries and packages\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Learning Libraries \n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU, CuDNNGRU, CuDNNLSTM\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
    "from keras.models import Model, load_model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from keras import backend as K\n",
    "from keras.engine import InputSpec, Layer\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2 Load/Read in Augmented Dataset without preprocessed reviews and Testing Dataset\n",
    "<a id=\"Load/Read in Augmented Dataset without preprocessed reviews and Testing Dataset\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in augmented dataset with unpreprocessed sentence column for training\n",
    "train = pd.read_csv(\n",
    "'/content/gdrive/My Drive/yelp-multiclass-sentiment-dataset/650K_LSTM_Repredicted_originalSent_Augment_Dataset.csv', sep=\",\")\n",
    "\n",
    "# read in testing dataset \n",
    "test = pd.read_csv(\n",
    "    '/content/gdrive/My Drive/yelp-multiclass-sentiment-dataset/test_data.csv', sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for na as some reviews have just be emptry and remove them as this'll affect tokenization \n",
    "train = train.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3 Tokenization \n",
    "<a id=\"Tokenization\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = list(train['text'].values)\n",
    "y = train['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = Tokenizer(lower = True, filters='')\n",
    "tk.fit_on_texts(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenized = tk.texts_to_sequences(train['text'])\n",
    "test_tokenized = tk.texts_to_sequences(test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 250\n",
    "X_train = pad_sequences(train_tokenized, maxlen = max_len)\n",
    "X_test = pad_sequences(test_tokenized, maxlen = max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.4 FastText Word Embedding Matrix\n",
    "<a id=\"FastText Word Embedding Matrix\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.4.1 Download FastText \n",
    "<a id=\"Download FastText\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.4.2 Unzip FastText Word Embeddings\n",
    "<a id=\"Unzip FastText Word Embeddings\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "zip_ref = zipfile.ZipFile('crawl-300d-2M.vec.zip', 'r')\n",
    "zip_ref.extractall('/content/gdrive/My Drive/FastText Embeddings/')\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_path = \"/content/gdrive/My Drive/FastText Embeddings/crawl-300d-2M.vec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 300\n",
    "max_features = len(tk.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.4.3 Constructing and Implementing FastText Word Embedding Matrix\n",
    "<a id=\"Constructing and Implementing FastText Word Embedding Matrix\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path))\n",
    "\n",
    "word_index = tk.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words + 1, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.5 One Hot Encode Training Dataset \n",
    "<a id=\"One Hot Encode Training Dataset\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "y_ohe = ohe.fit_transform(y.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.6 Ensemble Models Implementation\n",
    "<a id=\"Ensemble Models Implementation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model1(lr=0.0, lr_d=0.0, units=0, spatial_dr=0.0, kernel_size1=3, kernel_size2=2, dense_units=128, dr=0.1, conv_size=32):\n",
    "    file_path = \"/content/gdrive/My Drive/best_model.hdf5\"\n",
    "    check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n",
    "                                  save_best_only = True, mode = \"min\")\n",
    "    early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 3)\n",
    "    \n",
    "    inp = Input(shape = (max_len,))\n",
    "    x = Embedding(1153123, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n",
    "    x1 = SpatialDropout1D(spatial_dr)(x)\n",
    "\n",
    "    x_gru = Bidirectional(CuDNNGRU(units, return_sequences = True))(x1)\n",
    "    x1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
    "    avg_pool1_gru = GlobalAveragePooling1D()(x1)\n",
    "    max_pool1_gru = GlobalMaxPooling1D()(x1)\n",
    "    \n",
    "    x3 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
    "    avg_pool3_gru = GlobalAveragePooling1D()(x3)\n",
    "    max_pool3_gru = GlobalMaxPooling1D()(x3)\n",
    "    \n",
    "    x_lstm = Bidirectional(CuDNNLSTM(units, return_sequences = True))(x1)\n",
    "    x1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
    "    avg_pool1_lstm = GlobalAveragePooling1D()(x1)\n",
    "    max_pool1_lstm = GlobalMaxPooling1D()(x1)\n",
    "    \n",
    "    x3 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
    "    avg_pool3_lstm = GlobalAveragePooling1D()(x3)\n",
    "    max_pool3_lstm = GlobalMaxPooling1D()(x3)\n",
    "    \n",
    "    \n",
    "    x = concatenate([avg_pool1_gru, max_pool1_gru, avg_pool3_gru, max_pool3_gru,\n",
    "                    avg_pool1_lstm, max_pool1_lstm, avg_pool3_lstm, max_pool3_lstm])\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dr)(Dense(dense_units, activation='relu') (x))\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dr)(Dense(int(dense_units / 2), activation='relu') (x))\n",
    "    x = Dense(5, activation = \"softmax\")(x)\n",
    "    model = Model(inputs = inp, outputs = x)\n",
    "    model.compile(loss = \"categorical_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n",
    "    history = model.fit(X_train, y_ohe, batch_size = 128, epochs = 4, validation_split=0.1, \n",
    "                        verbose = 1, callbacks = [check_point, early_stop])\n",
    "    model = load_model(file_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = build_model1(lr = 1e-3, lr_d = 1e-10, units = 100, spatial_dr = 0.2, kernel_size1=3, kernel_size2=2, \n",
    "                      dense_units=32, dr=0.1, conv_size=32)\n",
    "model1.save('/content/gdrive/My Drive/best_model1.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = build_model1(lr = 1e-3, lr_d = 1e-10, units = 100, spatial_dr = 0.5, kernel_size1=3, kernel_size2=2, \n",
    "                      dense_units=64, dr=0.2, conv_size=32)\n",
    "model2.save('/content/gdrive/My Drive/best_model2.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model2(lr=0.0, lr_d=0.0, units=0, spatial_dr=0.0, kernel_size1=3, kernel_size2=2, dense_units=128, dr=0.1, conv_size=32):\n",
    "    file_path = \"/content/gdrive/My Drive/best_model.hdf5\"\n",
    "    check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n",
    "                                  save_best_only = True, mode = \"min\")\n",
    "    early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 3)\n",
    "\n",
    "    inp = Input(shape = (max_len,))\n",
    "    x = Embedding(1153123, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n",
    "    x1 = SpatialDropout1D(spatial_dr)(x)\n",
    "\n",
    "    x_gru = Bidirectional(CuDNNGRU(units, return_sequences = True))(x1)\n",
    "    x_lstm = Bidirectional(CuDNNLSTM(units, return_sequences = True))(x1)\n",
    "    \n",
    "    x_conv1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
    "    avg_pool1_gru = GlobalAveragePooling1D()(x_conv1)\n",
    "    max_pool1_gru = GlobalMaxPooling1D()(x_conv1)\n",
    "    \n",
    "    x_conv2 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
    "    avg_pool2_gru = GlobalAveragePooling1D()(x_conv2)\n",
    "    max_pool2_gru = GlobalMaxPooling1D()(x_conv2)\n",
    "    \n",
    "    \n",
    "    x_conv3 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
    "    avg_pool1_lstm = GlobalAveragePooling1D()(x_conv3)\n",
    "    max_pool1_lstm = GlobalMaxPooling1D()(x_conv3)\n",
    "    \n",
    "    x_conv4 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
    "    avg_pool2_lstm = GlobalAveragePooling1D()(x_conv4)\n",
    "    max_pool2_lstm = GlobalMaxPooling1D()(x_conv4)\n",
    "    \n",
    "    \n",
    "    x = concatenate([avg_pool1_gru, max_pool1_gru, avg_pool2_gru, max_pool2_gru,\n",
    "                    avg_pool1_lstm, max_pool1_lstm, avg_pool2_lstm, max_pool2_lstm])\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dr)(Dense(dense_units, activation='relu') (x))\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dr)(Dense(int(dense_units / 2), activation='relu') (x))\n",
    "    x = Dense(5, activation = \"softmax\")(x)\n",
    "    model = Model(inputs = inp, outputs = x)\n",
    "    model.compile(loss = \"categorical_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n",
    "    history = model.fit(X_train, y_ohe, batch_size = 128, epochs = 4, validation_split=0.1, \n",
    "                        verbose = 1, callbacks = [check_point, early_stop])\n",
    "    model = load_model(file_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = build_model2(lr = 1e-4, lr_d = 0, units = 100, spatial_dr = 0.5, kernel_size1=4, kernel_size2=3, \n",
    "                      dense_units=32, dr=0.1, conv_size=32)\n",
    "model3.save('/content/gdrive/My Drive/best_model3.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = build_model2(lr = 1e-3, lr_d = 0, units = 100, spatial_dr = 0.5, kernel_size1=3, kernel_size2=3, \n",
    "                      dense_units=64, dr=0.3, conv_size=32)\n",
    "model4.save('/content/gdrive/My Drive/best_model4.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = build_model2(lr = 1e-3, lr_d = 1e-7, units = 100, spatial_dr = 0.3, kernel_size1=3, kernel_size2=3, \n",
    "                      dense_units=64, dr=0.4, conv_size=64)\n",
    "model5.save('/content/gdrive/My Drive/best_model5.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.7 Loading Optimal Model Implementations\n",
    "<a id=\"Loading Optimal Model Implementations\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################## IMPORTANT ########################################################## \n",
    "\n",
    "# If the Google Colab Session crashes or runs into a Runtime error, then please run this code block to load in the presaved \n",
    "# optimal model for model1 to model5. This'll prevent the hassle of re-building the models, as they're computationally \n",
    "# expensive and can raise Out of Memory (OOM) error inside Google Colab\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "model1 = load_model('/content/gdrive/My Drive/best_model1.hdf5') # Model 1 Implementation\n",
    "model2 = load_model('/content/gdrive/My Drive/best_model2.hdf5') # Model 2 Implementation\n",
    "model3 = load_model('/content/gdrive/My Drive/best_model3.hdf5') # Model 3 Implementation\n",
    "model4 = load_model('/content/gdrive/My Drive/best_model4.hdf5') # Model 4 Implementation\n",
    "model5 = load_model('/content/gdrive/My Drive/best_model5.hdf5') # Model 5 Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Ensemble Testing Dataset Predictions\n",
    "<a id=\"Ensemble Testing Dataset Predictions\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = model1.predict(X_test, batch_size = 1024, verbose = 1)\n",
    "pred = pred1\n",
    "pred2 = model2.predict(X_test, batch_size = 1024, verbose = 1)\n",
    "pred += pred2\n",
    "pred3 = model3.predict(X_test, batch_size = 1024, verbose = 1)\n",
    "pred += pred3\n",
    "pred4 = model4.predict(X_test, batch_size = 1024, verbose = 1)\n",
    "pred += pred4\n",
    "pred5 = model5.predict(X_test, batch_size = 1024, verbose = 1)\n",
    "pred += pred5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [1,2,3,4,5]\n",
    "\n",
    "predicted_labels = []\n",
    "\n",
    "for prediction in predictions:\n",
    "  predicted_labels.append(labels[np.round(np.argmax(prediction)).astype(int)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Create and export output file predict_label.csv \n",
    "<a id=\"Create and export output file predict_label.csv \"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame({'test_id' : test['test_id'],\n",
    "                       'label' : predicted_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv('predict_label.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This'll allow exporting/downloading CSV files from Google Colab to the Local Machine \n",
    "from google.colab import files\n",
    "files.download('predict_label.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "<a id=\"references\"></a>\n",
    "\n",
    "- Calin, Timbus. (2019, May 12), \"How to fix name“ Embedding is not defined” in Keras\" Stack Overflow [Online] Available at: https://stackoverflow.com/questions/56097089/how-to-fix-name-embedding-is-not-defined-in-keras (Accessed: 12/10/2019) \n",
    "- cjbrog. (2017, April 9), \"Deprecation warnings from sklearn\" Stack Overflow [Online] Available at: https://stackoverflow.com/questions/43302400/deprecation-warnings-from-sklearn (Accessed: 12/10/2019) \n",
    "- Dieter. (2019), “How To: Preprocessing for GloVe Part1: EDA” kaggle [Online] Available at: https://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part1-eda (Accessed: 17/10/2019)\n",
    "- Dieter. (2019), “How To: Preprocessing for GloVe Part2: Usage” kaggle [Online] Available at: https://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part2-usage (Accessed: 17/10/2019)\n",
    "- Kashyap. (2017, March 4), \"Why can't I use preprocessing module in Keras?\" Stack Overflow [Online] Available at: https://stackoverflow.com/questions/42598630/why-cant-i-use-preprocessing-module-in-keras (Accessed: 12/10/2019) \n",
    "- Lukyanenko, Andrew. (2019), “Movie Review Sentiment Analysis EDA and models” kaggle [Online] Available at: https://www.kaggle.com/artgor/movie-review-sentiment-analysis-eda-and-models (Accessed: 25/10/2019) \n",
    "- maxpumperla. (2019, February 14), \"Name 'LSTM' is not defined \" GitHub maxpumperla/hyperas [Online] Available at: https://github.com/maxpumperla/hyperas/issues/199 (Accessed: 12/10/2019)  \n",
    "- micts. (2018, August 28), \"Do I have to preprocess my new data for a prediction, if I have used preprocessing for building the model?\" StackExchange Cross Validated [Online] Available at: https://stats.stackexchange.com/questions/364382/do-i-have-to-preprocess-my-new-data-for-a-prediction-if-i-have-used-preprocessi (Accessed: 11/10/2019) \n",
    "- Sonthalia, Akash. (2019 September 30), \"name 'Sequential' is not defined\" Stack Overflow [Online] Available at: https://stackoverflow.com/questions/57021088/name-sequential-is-not-defined (Accessed: 12/10/2019)\n",
    "- TensorFlow Core r2.0. (2019), \"tf.keras.layers.SpatialDropout1D\" TensorFlow [Online] Available at: https://www.tensorflow.org/api_docs/python/tf/keras/layers/SpatialDropout1D (Accessed: 12/10/2019) \n",
    "- user4815162342. (2012, November 2), \"'str' object has no attribute 'punctuation' [closed]\" Stack Overflow [Online] Available at: https://stackoverflow.com/questions/13197913/str-object-has-no-attribute-punctuation (Accessed: 11/10/2019)\n",
    "- Valdenegro, Matias. (2019, June 20), \"Keras EarlyStopping is not recognized\" Stack Overflow [Online] Available at: https://stackoverflow.com/questions/56687658/keras-earlystopping-is-not-recognized (Accessed: 12/10/2019) \n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "FeP8NbuJ9duM",
    "3Z74mHir9dux",
    "spHCbaF29du-",
    "maNrxmkK9dvI",
    "uNv6SzfO9dv0"
   ],
   "machine_shape": "hm",
   "name": "Copy of Copy of Logistic Regression-UniDirectional LSTM.ipynb",
   "provenance": [
    {
     "file_id": "13PRyUI7RS5OMj928CV4L75r1H8eO2JPz",
     "timestamp": 1572169166601
    },
    {
     "file_id": "1q1yWHV6E1yHBAy8Y_xgi1_6T6yHu4d8o",
     "timestamp": 1572167363350
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
